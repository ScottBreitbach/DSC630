# https://www.datacamp.com/community/tutorials/decision-trees-R

## RANDOM FORESTS

library(MASS)
data(package="MASS")
boston<-Boston
dim(boston)
names(boston)

require(randomForest)

set.seed(101)
train = sample(1:nrow(boston), 300)

rf.boston = randomForest(medv~., data = boston, subset = train)
rf.boston

oob.err = double(13)
test.err = double(13)
for(mtry in 1:13){
  fit = randomForest(medv~., data = boston, subset=train, mtry=mtry, ntree = 350)
  oob.err[mtry] = fit$mse[350]
  pred = predict(fit, boston[-train,])
  test.err[mtry] = with(boston[-train,], mean( (medv-pred)^2 ))
}

matplot(1:mtry, cbind(oob.err, test.err), pch = 23, col = c("red", "blue"), type = "b", ylab="Mean Squared Error")
legend("topright", legend = c("OOB", "Test"), pch = 23, col = c("red", "blue"))


## BOOSTING
if (!require(gbm)) install.packages('gbm')
library(gbm) # Gradient Boosting Model

boost.boston = gbm(medv~., data = boston[train,], distribution = "gaussian", n.trees = 10000, shrinkage = 0.01, interaction.depth = 4)
summary(boost.boston)

plot(boost.boston,i="lstat")
plot(boost.boston,i="rm")

n.trees = seq(from = 100, to = 10000, by = 100)
predmat = predict(boost.boston, newdata = boston[-train,], n.trees = n.trees)
dim(predmat)

boost.err = with(boston[-train,], apply( (predmat - medv)^2, 2, mean) )
plot(n.trees, boost.err, pch = 23, ylab = "Mean Squared Error", xlab = "# Trees", main = "Boosting Test Error")
abline(h = min(test.err), col = "red")

min(test.err)
min(boost.err)






